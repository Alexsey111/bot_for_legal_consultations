"""
database.py
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ì–ö –†–§
–†–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –ø—É–Ω–∫—Ç–∞–º —Å—Ç–∞—Ç–µ–π —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
"""

import shutil
from pathlib import Path
from typing import List, Optional, Dict, Tuple
import re

import structlog

from langchain_core.documents import Document
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—á–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ì–ö –†–§
from gk_structure import get_chapter_for_article, determine_gk_part

log = structlog.get_logger()

# ================= CONFIG =================
PERSIST_DIRECTORY = "./chroma_legal_db"
COLLECTION_NAME = "gk_rf_articles"

# –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –¥–µ—à–µ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è embeddings
EMBEDDING_MODEL = "text-embedding-3-small"

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–∞–∑–±–∏–µ–Ω–∏—è
MIN_CHUNK_SIZE = 200
IDEAL_CHUNK_SIZE = 1200
MAX_CHUNK_SIZE = 2500

# –î–ª—è —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ –¥–ª–∏–Ω–Ω—ã—Ö –ø—É–Ω–∫—Ç–æ–≤
FORCE_SPLIT_AT = 2500
FORCE_SPLIT_OVERLAP = 400


# ================= UTILS =================


def extract_article_info(text: str) -> Optional[Tuple[str, str]]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ–º–µ—Ä –∏ –Ω–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç—å–∏
    Returns: (–Ω–æ–º–µ—Ä, –Ω–∞–∑–≤–∞–Ω–∏–µ) –∏–ª–∏ None
    """
    match = re.search(r"–°—Ç–∞—Ç—å—è\s+(\d+(?:\.\d+)?)\.\s*([^\n]+)", text)
    if match:
        return match.group(1), match.group(2).strip()
    return None


def extract_chapter_info(text: str) -> Optional[Tuple[str, str]]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ–º–µ—Ä –∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –≥–ª–∞–≤—ã
    Returns: (–Ω–æ–º–µ—Ä, –Ω–∞–∑–≤–∞–Ω–∏–µ) –∏–ª–∏ None
    """
    match = re.search(r"–ì–ª–∞–≤–∞\s+(\d+)\.\s*([^\n]+)", text)
    if match:
        return match.group(1), match.group(2).strip()
    return None


def split_article_by_points(article_text: str, article_num: str, article_title: str) -> List[Dict]:
    """
    –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø: –†–∞–∑–±–∏–≤–∞–µ—Ç —Å—Ç–∞—Ç—å—é –Ω–∞ –ø—É–Ω–∫—Ç—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º lookahead split
    Returns: —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–∞–∂–¥–æ–º –ø—É–Ω–∫—Ç–µ
    """
    # –£–±–∏—Ä–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞—Ç—å–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞
    text_without_header = re.sub(r"–°—Ç–∞—Ç—å—è\s+\d+(?:\.\d+)?\.\s*[^\n]+\n*", "", article_text, count=1).strip()
    
    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º lookahead split –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–≥–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è –ø–æ –ø—É–Ω–∫—Ç–∞–º
    # (?=\n?\d+\.\s) –æ–∑–Ω–∞—á–∞–µ—Ç "—Ä–∞–∑–¥–µ–ª–∏ –ø–µ—Ä–µ–¥ –∫–∞–∂–¥—ã–º –Ω–æ–º–µ—Ä–æ–º –ø—É–Ω–∫—Ç–∞"
    # \n? –¥–µ–ª–∞–µ—Ç –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º - —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –ª—é–±—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    points_split = re.split(r'(?=\n?\d+\.\s)', text_without_header)

    result = []
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –±–ª–æ–∫
    for block in points_split:
        block = block.strip()
        if not block:
            continue
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä –ø—É–Ω–∫—Ç–∞ –∏ —Ç–µ–∫—Å—Ç
        match = re.match(r'\s*(\d+)\.\s+(.*)', block, re.DOTALL)

        if match:
            point_num = match.group(1)
            point_text = match.group(2).strip()

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø–æ–¥–ø—É–Ω–∫—Ç–æ–≤ (–∞, –±, –≤) –∏–ª–∏ —Å–∫–æ–±–æ—á–Ω—ã—Ö (1, 2, 3)
            has_letter_subpoints = bool(re.search(r'\n\s*[–∞-—è—ë]\)', point_text, re.IGNORECASE))
            has_digit_subpoints = bool(re.search(r'\n\s*\d+\)', point_text))
            has_abzac = bool(re.search(r'–∞–±–∑–∞—Ü', point_text, re.IGNORECASE))

            has_subpoints = has_letter_subpoints or has_digit_subpoints or has_abzac

            result.append({
                "point_num": point_num,
                "text": point_text,
                "has_subpoints": has_subpoints,
                "is_full_article": False
            })
        else:
            # –ë–ª–æ–∫ –±–µ–∑ –Ω–æ–º–µ—Ä–∞ - —ç—Ç–æ –ø—Ä–µ–∞–º–±—É–ª–∞ –∏–ª–∏ —Ç–µ–∫—Å—Ç –±–µ–∑ –Ω—É–º–µ—Ä–∞—Ü–∏–∏
            # –î–æ–±–∞–≤–ª—è–µ–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É –ø—É–Ω–∫—Ç—É –∏–ª–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π –±–ª–æ–∫
            if result:
                # –î–æ–±–∞–≤–ª—è–µ–º –∫ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É –ø—É–Ω–∫—Ç—É
                result[-1]["text"] += "\n\n" + block
            else:
                # –ï—Å–ª–∏ —ç—Ç–æ –ø–µ—Ä–≤—ã–π –±–ª–æ–∫ –±–µ–∑ –Ω—É–º–µ—Ä–∞—Ü–∏–∏ - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
                log.debug("article_no_points", article_num=article_num)
                return [{
                    "point_num": None,
                    "text": block,
                    "has_subpoints": False,
                    "is_full_article": True
                }]

    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø—É–Ω–∫—Ç–æ–≤ - —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ü–µ–ª–∏–∫–æ–º
    if not result:
        log.debug("article_no_points_found", article_num=article_num)
        return [{
            "point_num": None,
            "text": text_without_header,
            "has_subpoints": False,
            "is_full_article": True
        }]

    log.debug("article_points_found", article_num=article_num, count=len(result))
    return result


def extract_keywords(text: str) -> List[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã"""
    keywords = []
    
    legal_terms = [
        "–¥–æ–≥–æ–≤–æ—Ä", "–æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ", "–ø—Ä–∞–≤–æ", "–æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç—å", "–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å",
        "—Å—Ç–æ—Ä–æ–Ω–∞", "–ª–∏—Ü–æ", "–∏–º—É—â–µ—Å—Ç–≤–æ", "—Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å", "–≤–ª–∞–¥–µ–Ω–∏–µ",
        "—Å–¥–µ–ª–∫–∞", "—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ", "–ø—Ä–æ–¥–∞–≤–µ—Ü", "–ø–æ–∫—É–ø–∞—Ç–µ–ª—å", "–∞—Ä–µ–Ω–¥–∞—Ç–æ—Ä",
        "–∏—Å–∫–æ–≤–∞—è –¥–∞–≤–Ω–æ—Å—Ç—å", "–≤–æ–∑–º–µ—â–µ–Ω–∏–µ", "—É–±—ã—Ç–∫–∏", "–ø—Ä–æ—Ü–µ–Ω—Ç—ã", "–Ω–µ—É—Å—Ç–æ–π–∫–∞",
        "–∑–∞–ª–æ–≥", "–ø–æ—Ä—É—á–∏—Ç–µ–ª—å—Å—Ç–≤–æ", "–≥–∞—Ä–∞–Ω—Ç–∏—è", "—Ä–∞—Å—Ç–æ—Ä–∂–µ–Ω–∏–µ", "–Ω–µ–¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å"
    ]
    
    text_lower = text.lower()
    for term in legal_terms:
        if term in text_lower:
            keywords.append(term)
    
    return keywords[:10]


def extract_article_references(text: str) -> List[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥—Ä—É–≥–∏–µ —Å—Ç–∞—Ç—å–∏"""
    patterns = [
        r"—Å—Ç–∞—Ç—å–∏?\s+(\d+)",
        r"—Å—Ç–∞—Ç—å–µ\s+(\d+)",
        r"—Å—Ç–∞—Ç—å—é\s+(\d+)",
        r"—Å—Ç\.\s*(\d+)",
    ]
    
    references = set()
    for pattern in patterns:
        matches = re.finditer(pattern, text, re.IGNORECASE)
        for match in matches:
            references.add(match.group(1))
    
    return sorted(list(references))


def chunk_long_text(text: str, max_size: int, overlap: int) -> List[str]:
    """–†–∞–∑–±–∏–≤–∞–µ—Ç –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏"""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=max_size,
        chunk_overlap=overlap,
        length_function=len,
        separators=["\n\n", "\n", ". ", ", ", " ", ""]
    )
    
    return splitter.split_text(text)


# ================= MAIN PROCESSING =================

def process_gk_text(text: str, law_name: str = "–ì—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π –∫–æ–¥–µ–∫—Å –†–§") -> List[Document]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –ì–ö –†–§ –∏ —Å–æ–∑–¥–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã

    –ö–†–ò–¢–ò–ß–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ article_order_index
    –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    """
    log.info("processing_start", law_name=law_name)
    
    documents = []
    # –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Å—á–µ—Ç—á–∏–∫ –ø–æ—Ä—è–¥–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Å—Ç—Ä–æ–≥–æ–≥–æ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    article_order_index = 0
    
    # –ò–°–ü–†–ê–í–õ–ï–ù–û: —É–ª—É—á—à–µ–Ω–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ —Å—Ç–∞—Ç—å—è–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º split –ø–æ lookahead
    # –≠—Ç–æ –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–π –ø–æ–¥—Ö–æ–¥, —á–µ–º findall, —Ç.–∫. –Ω–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–Ω—É—Ç—Ä–∏ —Å—Ç–∞—Ç—å–∏
    articles = re.split(r'(?=–°—Ç–∞—Ç—å—è\s+\d+(?:\.\d+)?\.)', text)
    articles = [a.strip() for a in articles if a.strip().startswith("–°—Ç–∞—Ç—å—è")]

    log.info("articles_found", count=len(articles))
    
    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ç–∏—á–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≥–ª–∞–≤
    # –≠—Ç–æ –Ω–∞–¥–µ–∂–Ω–µ–µ, —á–µ–º —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç–∞—Ç–µ–π
    log.info("üìã Using static GK structure mapping for chapter detection")

    for article_block in articles:
        if not article_block.strip():
            continue
        
        article_info = extract_article_info(article_block)
        if not article_info:
            continue
        
        article_num, article_title = article_info
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ç–∏—á–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —á–∞—Å—Ç–∏ –∏ –≥–ª–∞–≤—ã
        chapter_num, chapter_title, part_num = get_chapter_for_article(article_num)

        log.info(
            "processing_article",
            article_num=article_num,
            article_title=article_title[:50],
            part_num=part_num,
            chapter_num=chapter_num or "N/A",
            chapter_title=chapter_title
        )
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏ —Å—Å—ã–ª–∫–∏
        keywords = extract_keywords(article_block)
        references = extract_article_references(article_block)
        
        # –ì–õ–ê–í–ù–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –†–∞–∑–±–∏–≤–∞–µ–º —Å—Ç–∞—Ç—å—é –Ω–∞ –ø—É–Ω–∫—Ç—ã
        points = split_article_by_points(article_block, article_num, article_title)
        
        # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—É–Ω–∫—Ç–∞
        for point_data in points:
            point_num = point_data["point_num"]
            point_text = point_data["text"]
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è —á–∞–Ω–∫–∞
            if point_num:
                full_content = f"–°—Ç–∞—Ç—å—è {article_num}. {article_title}\n\n{point_num}. {point_text}"
            else:
                full_content = f"–°—Ç–∞—Ç—å—è {article_num}. {article_title}\n\n{point_text}"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É
            content_length = len(full_content)
            
            # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π - —Ä–∞–∑–±–∏–≤–∞–µ–º
            if content_length > FORCE_SPLIT_AT:
                log.info("article_point_too_long", article_num=article_num, point_num=point_num, length=content_length)
                
                chunks = chunk_long_text(point_text, MAX_CHUNK_SIZE, FORCE_SPLIT_OVERLAP)
                
                for chunk_idx, chunk in enumerate(chunks):
                    chunk_content = f"–°—Ç–∞—Ç—å—è {article_num}. {article_title}\n\n{point_num}. {chunk}"
                    
                    doc = create_document(
                        content=chunk_content,
                        law_name=law_name,
                        article_num=article_num,
                        article_title=article_title,
                        point_num=point_num,
                        part_num=part_num,
                        chapter_num=chapter_num,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ç–∏—á–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥
                        chapter_title=chapter_title,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ç–∏—á–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥
                        keywords=keywords,
                        references=references,
                        has_subpoints=point_data["has_subpoints"],
                        is_full_article=point_data["is_full_article"],
                        total_points=len(points),
                        chunk_index=chunk_idx,
                        total_chunks=len(chunks),
                        article_order_index=article_order_index
                    )
                    documents.append(doc)
                    article_order_index += 1  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            else:
                # –°–æ–∑–¥–∞–µ–º –æ–±—ã—á–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
                doc = create_document(
                    content=full_content,
                    law_name=law_name,
                    article_num=article_num,
                    article_title=article_title,
                    point_num=point_num,
                    part_num=part_num,
                    chapter_num=chapter_num,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ç–∏—á–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥
                    chapter_title=chapter_title,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ç–∏—á–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥
                    keywords=keywords,
                    references=references,
                    has_subpoints=point_data["has_subpoints"],
                    is_full_article=point_data["is_full_article"],
                    total_points=len(points),
                    article_order_index=article_order_index
                )
                documents.append(doc)
                article_order_index += 1  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    
    log.info("documents_created", count=len(documents), last_order_index=article_order_index - 1)
    
    return documents


def create_document(
    content: str,
    law_name: str,
    article_num: str,
    article_title: str,
    point_num: Optional[str],
    part_num: int,
    chapter_num: Optional[str],
    chapter_title: Optional[str],
    keywords: List[str],
    references: List[str],
    has_subpoints: bool,
    is_full_article: bool,
    total_points: int,
    chunk_index: int = 0,
    total_chunks: int = 1,
    article_order_index: int = 0
) -> Document:
    """
    –°–æ–∑–¥–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç —Å –±–æ–≥–∞—Ç—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏

    Args:
        article_order_index: –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å –ø–æ—Ä—è–¥–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                            –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏
    """
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Å—ã–ª–∫–∏
    if point_num and not is_full_article:
        reference = f"—Å—Ç. {article_num} –ø. {point_num} –ì–ö –†–§"
        full_reference = f"–°—Ç–∞—Ç—å—è {article_num} –ø—É–Ω–∫—Ç {point_num} –ì—Ä–∞–∂–¥–∞–Ω—Å–∫–æ–≥–æ –∫–æ–¥–µ–∫—Å–∞ –†–§ (—á–∞—Å—Ç—å {part_num})"
    else:
        reference = f"—Å—Ç. {article_num} –ì–ö –†–§"
        full_reference = f"–°—Ç–∞—Ç—å—è {article_num} –ì—Ä–∞–∂–¥–∞–Ω—Å–∫–æ–≥–æ –∫–æ–¥–µ–∫—Å–∞ –†–§ (—á–∞—Å—Ç—å {part_num})"
    
    # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
    # –§–æ—Ä–º–∞—Ç: article_num_pointNum_chunkIndex (–Ω–∞–ø—Ä–∏–º–µ—Ä: "454_1_0", "454_full_0")
    point_id = point_num if point_num else "full"
    doc_id = f"{article_num}_{point_id}_{chunk_index}"

    # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    metadata = {
        # –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è
        "doc_id": doc_id,  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
        "law_name": law_name,
        "article_num": article_num,
        "article_title": article_title,
        "point_num": point_num if point_num else "full",
        
        # –ü–æ—Ä—è–¥–æ–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ (–∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è —Å—Ç—Ä–æ–≥–æ–≥–æ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
        "article_order_index": article_order_index,  # –ì–ª–æ–±–∞–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞

        # –ò–µ—Ä–∞—Ä—Ö–∏—è
        "part": part_num,
        "chapter": chapter_num if chapter_num else "unknown",
        "chapter_title": chapter_title if chapter_title else "unknown",
        
        # –î–ª—è —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        "reference": reference,
        "full_reference": full_reference,
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å—Ç–∞—Ç—å–∏
        "has_subpoints": has_subpoints,
        "is_full_article": is_full_article,
        "total_points": total_points,
        
        # –°–µ–º–∞–Ω—Ç–∏–∫–∞
        "keywords": ",".join(keywords) if keywords else "",
        "references": ",".join(references) if references else "",
        
        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ
        "chunk_type": "full_article" if is_full_article else "point",
        "chunk_index": chunk_index,
        "total_chunks": total_chunks,
        "char_length": len(content)
    }
    
    return Document(page_content=content, metadata=metadata)


# ================= MAIN CLASS =================

class LegalVectorDB:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î –¥–ª—è –ì–ö –†–§"""
    
    def __init__(self):
        self.embeddings = OpenAIEmbeddings(
            model=EMBEDDING_MODEL,
            chunk_size=1000
        )
        self.vector_db = None
        log.info("embedding_model_initialized", model=EMBEDDING_MODEL)
    
    def rebuild_from_file(self, file_path: str, law_name: str = "–ì—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π –∫–æ–¥–µ–∫—Å –†–§"):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ì–ö –†–§ –∏–∑ —Ñ–∞–π–ª–∞ –∏ —Å–æ–∑–¥–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î"""
        
        log.info("database_rebuild_start")
        
        # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
        log.info("reading_file", path=file_path)
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()
        
        file_size_mb = len(text) / (1024 * 1024)
        log.info("file_size", mb=f"{file_size_mb:.2f}", chars=len(text))
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç
        documents = process_gk_text(text, law_name)
        
        if not documents:
            raise ValueError("‚ùå No documents created!")
        
        log.info("documents_created", count=len(documents))
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—É—é –ë–î
        if Path(PERSIST_DIRECTORY).exists():
            log.info("removing_old_database")
            shutil.rmtree(PERSIST_DIRECTORY)
        
        # –°–æ–∑–¥–∞–µ–º –ë–î –ø–∞–∫–µ—Ç–∞–º–∏
        log.info("creating_vector_database")
        
        batch_size = 100
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            
            if i == 0:
                self.vector_db = Chroma.from_documents(
                    documents=batch,
                    embedding=self.embeddings,
                    persist_directory=PERSIST_DIRECTORY,
                    collection_name=COLLECTION_NAME,
                )
            else:
                self.vector_db.add_documents(batch)
            
            progress = min(i + batch_size, len(documents))
            percentage = (progress / len(documents)) * 100
            log.info("progress", current=progress, total=len(documents), percent=f"{percentage:.1f}%")
        
        log.info("database_rebuild_complete", chunks=len(documents))
    
    def _get_collection_count(self) -> int:
        """
        –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        –ò–∑–±–µ–≥–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è private API _collection –∫–æ–≥–¥–∞ —ç—Ç–æ –≤–æ–∑–º–æ–∂–Ω–æ
        """
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –≤—Å–µ ID –∏ –ø–æ—Å—á–∏—Ç–∞—Ç—å –∏—Ö
            # –≠—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—É–±–ª–∏—á–Ω—ã–π API, –Ω–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞—Ç—Ä–∞—Ç–Ω–æ –¥–ª—è –±–æ–ª—å—à–∏—Ö –ë–î
            result = self.vector_db.get(include=["ids"])
            if result and "ids" in result:
                return len(result["ids"])
        except Exception as e:
            log.warning("count_via_public_api_failed", error=str(e)[:100])

        # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º _collection —Å —è–≤–Ω—ã–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ–º
        try:
            if hasattr(self.vector_db, "_collection"):
                log.warning("using_private_api_fallback", method="_collection.count()")
                return self.vector_db._collection.count()
        except Exception as e:
            log.error("count_via_private_api_failed", error=str(e)[:100])

        return 0

    def load(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –ë–î"""
        if not Path(PERSIST_DIRECTORY).exists():
            raise FileNotFoundError(
                f"‚ùå Database not found at {PERSIST_DIRECTORY}\n"
                "Please run: python ingest_data.py first!"
            )
        
        log.info("loading_database", path=PERSIST_DIRECTORY)
        self.vector_db = Chroma(
            collection_name=COLLECTION_NAME,
            persist_directory=PERSIST_DIRECTORY,
            embedding_function=self.embeddings,
        )
        
        count = self._get_collection_count()
        log.info("database_loaded", chunks=count)
    
    def similarity_search(self, query: str, k: int = 10, use_mmr: bool = True, fetch_k: int = 20) -> List[Document]:
        """
        –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

        Args:
            query: –ó–∞–ø—Ä–æ—Å
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞
            use_mmr: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å MMR (Maximal Marginal Relevance) –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
            fetch_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è MMR (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å >= k)

        Returns:
            –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        if not self.vector_db:
            raise RuntimeError("‚ùå Database not loaded!")
        
        if use_mmr:
            # MMR: –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ–º
            # lambda_mult=0.5: 50% —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å, 50% —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
            return self.vector_db.max_marginal_relevance_search(
                query=query,
                k=k,
                fetch_k=fetch_k,
                lambda_mult=0.5
            )
        else:
            # –û–±—ã—á–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
            return self.vector_db.similarity_search(query, k=k)
    
    def similarity_search_with_score(self, query: str, k: int = 10) -> List[Tuple[Document, float]]:
        """
        –ü–æ–∏—Å–∫ —Å –æ—Ü–µ–Ω–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏

        –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: MMR (max_marginal_relevance_search) –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç scores,
        –ø–æ—ç—Ç–æ–º—É –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ü–µ–Ω–æ–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±—ã—á–Ω—ã–π similarity_search_with_score.
        –î–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ similarity_search(use_mmr=True).
        """
        if not self.vector_db:
            raise RuntimeError("‚ùå Database not loaded!")
        
        return self.vector_db.similarity_search_with_score(query, k=k)
    
    def get_article_by_number(self, article_num: str) -> List[Document]:
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –ø—É–Ω–∫—Ç—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç–∞—Ç—å–∏"""
        if not self.vector_db:
            raise RuntimeError("‚ùå Database not loaded!")
        
        results = self.vector_db.get(
            where={"article_num": article_num}
        )
        
        return [
            Document(page_content=text, metadata=meta)
            for text, meta in zip(results["documents"], results["metadatas"])]
        
    
    def get_all_documents(self) -> List[Document]:
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"""
        if not self.vector_db:
            raise RuntimeError("‚ùå Database not loaded!")
        
        data = self.vector_db.get()
        return [
            Document(page_content=text, metadata=meta)
            for text, meta in zip(data["documents"], data["metadatas"])]
        
    
    def get_stats(self) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ë–î"""
        if not self.vector_db:
            raise RuntimeError("‚ùå Database not loaded!")
        
        all_docs = self.get_all_documents()
        
        articles = set()
        points = 0
        full_articles = 0
        doc_ids = set()
        order_indices = []
        
        for doc in all_docs:
            articles.add(doc.metadata.get("article_num"))
            doc_ids.add(doc.metadata.get("doc_id"))
            order_idx = doc.metadata.get("article_order_index")
            if order_idx is not None:
                order_indices.append(order_idx)

            if doc.metadata.get("chunk_type") == "point":
                points += 1
            else:
                full_articles += 1
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –ø–æ—Ä—è–¥–∫–∞
        order_integrity = "unknown"
        if order_indices:
            expected_indices = sorted(set(order_indices))
            actual_indices = sorted(order_indices)
            if expected_indices == actual_indices:
                order_integrity = "valid"
            else:
                order_integrity = "invalid"

        return {
            "total_chunks": len(all_docs),
            "unique_articles": len(articles),
            "unique_doc_ids": len(doc_ids),
            "point_chunks": points,
            "full_article_chunks": full_articles,
            "order_integrity": order_integrity,
            "max_order_index": max(order_indices) if order_indices else 0,
        }
